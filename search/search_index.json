{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs \u00b6 For full documentation visit mkdocs.org . Nama : Ahmad Addin Wirayudha AA NIM : 180411100045 Prodi : Teknik Informatika Kelas : Penambangan Data - C MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Index"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org . Nama : Ahmad Addin Wirayudha AA NIM : 180411100045 Prodi : Teknik Informatika Kelas : Penambangan Data - C MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Welcome to MkDocs"},{"location":"Mengukur/","text":"Mengukur Jarak Data \u00b6 Mengukur Jarak Tipe Numerik \u00b6 Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya : Minkowski Distance \u00b6 Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ dimana mm adalah bilangan riel positif dan xixi dan $ y_i$ adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan distance \u00b6 Manhattan distance adalah kasus khusus dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ Euclidean distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ Weighted euclidean distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana wi adalah bobot yang diberikan pada atribut ke i. Chord distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ dimana \u2225x\u22252\u2016x\u20162 adalah $$ L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum { i = 1 }^{ n }x_{i}^{2}} $$ Mahalanobis distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ diman S adalah matrik covariance data. Cosine measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan $$ Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } $$ dimana \u2225y\u22252\u2016y\u20162 adalah Euclidean norm dari vektor $$ y=(y_{1} , y_{2} , \\dots , y_{n} ) $$ didefinisikan dengan $$ |y|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } $$ Pearson correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan $$ Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } $$ The Pearson correlation kelemahannya adalah sensitif terhadap outlier. Mengukur Jarak Atribut Binary \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Atribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d722\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan jj adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe Categorical \u00b6 Overlay Metric \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y)ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana C adalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ dimana probabilitas keanggotaan kelas diestimasi dengan P(c|x)P(c|x) dan P(c|y)P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Mengukur Jarak Tipe Ordinal \u00b6 Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$. Menghitung Jarak Tipe Campuran \u00b6 Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0] .Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, dfij=\u2225xif\u2212xjf\u2225maxhxhf\u2212minhxhfdijf=\u2016xif\u2212xjf\u2016maxhxhf\u2212minhxhf, di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan zif=rif\u22121Mf\u22121zif=rif\u22121Mf\u22121 , dan perlakukan zifzif sebagai numerik. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Menghitung Jarak"},{"location":"Mengukur/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"Mengukur/#mengukur-jarak-tipe-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya :","title":"Mengukur Jarak Tipe Numerik"},{"location":"Mengukur/#minkowski-distance","text":"Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ dimana mm adalah bilangan riel positif dan xixi dan $ y_i$ adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance"},{"location":"Mengukur/#manhattan-distance","text":"Manhattan distance adalah kasus khusus dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$","title":"Manhattan distance"},{"location":"Mengukur/#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance"},{"location":"Mengukur/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$","title":"Average Distance"},{"location":"Mengukur/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana wi adalah bobot yang diberikan pada atribut ke i.","title":"Weighted euclidean distance"},{"location":"Mengukur/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ dimana \u2225x\u22252\u2016x\u20162 adalah $$ L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum { i = 1 }^{ n }x_{i}^{2}} $$","title":"Chord distance"},{"location":"Mengukur/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ diman S adalah matrik covariance data.","title":"Mahalanobis distance"},{"location":"Mengukur/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan $$ Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } $$ dimana \u2225y\u22252\u2016y\u20162 adalah Euclidean norm dari vektor $$ y=(y_{1} , y_{2} , \\dots , y_{n} ) $$ didefinisikan dengan $$ |y|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } $$","title":"Cosine measure"},{"location":"Mengukur/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan $$ Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } $$ The Pearson correlation kelemahannya adalah sensitif terhadap outlier.","title":"Pearson correlation"},{"location":"Mengukur/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Atribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d722\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan jj adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary"},{"location":"Mengukur/#mengukur-jarak-tipe-categorical","text":"","title":"Mengukur Jarak Tipe Categorical"},{"location":"Mengukur/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y)ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric"},{"location":"Mengukur/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana C adalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ dimana probabilitas keanggotaan kelas diestimasi dengan P(c|x)P(c|x) dan P(c|y)P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)"},{"location":"Mengukur/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"Minimum Risk Metric (MRM)"},{"location":"Mengukur/#mengukur-jarak-tipe-ordinal","text":"Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$.","title":"Mengukur Jarak Tipe Ordinal"},{"location":"Mengukur/#menghitung-jarak-tipe-campuran","text":"Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0] .Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, dfij=\u2225xif\u2212xjf\u2225maxhxhf\u2212minhxhfdijf=\u2016xif\u2212xjf\u2016maxhxhf\u2212minhxhf, di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan zif=rif\u22121Mf\u22121zif=rif\u22121Mf\u22121 , dan perlakukan zifzif sebagai numerik. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Menghitung Jarak Tipe Campuran"},{"location":"keterangan/","text":"Statistik deksriptif adalah sebuah representasi keseluruhan himpunan data spesifik dengan memberikan ringkasan pendek tentang sampel dan ukuran data. Kegiatan dalam statistik deskriptif meliputi pengumpulan, pengelompokan dan pengolahan data yang selanjutnya akan menghasilkan ukuran-ukuran statistik agar data lebih mudah dibaca dan dipahami. Maka data dapat diringkas dalam bentuk tabulasi atau disajikan dalam bentuk grafik atau diagram, beberapa yang akan kita bahas yaitu : Mean(Rata-rata) \u00b6 Mean adalah rata-rata dari semua angka , yang didapat dengan menjumlahkan data seluruh individu dalam kelompok itu,kemudian dibagi dengan jumlah individu yang ada pada kelompok tersebut. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ x bar = Rata-rata x = Data ke-n n = Banyaknya data Median \u00b6 Median adalah nilai pusat yang terletak ditengah data yang telah diurutkan dari terkecil sampai terbesar. \u200b untuk data tunggal : $$ Me=\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap \\ $$ \u200b untuk data kelompok : $$ M e=x_{i j}+\\left(\\frac{\\frac{n}{2}-f_{k i j}}{f_{i}}\\right) p $$ Me = Median xi = batas bawah median n = jumlah data fkij = frekuensi kumulatif data di bawah kelas median fi = frekuensi data pada kelas median p = panjang interval kelas Modus \u00b6 Modus adalah data yang paling banyak ditemukan atau nilai yang sering muncul (frekuensi terbanyak). $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Mo = modus Tb = tepi bawah p = panjang interval b1 = selisih frekuensi antara elemen modus dengan elemen sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya Standar Deviasi & Varian \u00b6 Standar deviasi adalah nilai statistik yang digunakan untuk menentukan bagaimana sebaran data dalam sampel, dan seberapa dekat titik data individu pada rata-rata. Nilai sampel Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Varian adalah jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ xi = titik data x bar = Rata-rata n = banyak nya data Skewness \u00b6 Skewness adalah derajat ketidaksamaan atau ukuran kemiringan suatu distribusi yang digunakan untuk menentukan sejauhmana perbedaan suatu distribusi dengan distribusi normal. $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ xi = titik data x bar = Rata-rata dari distribusi n = banyak titik dalam distribusi o = standar deviasi Kuartil \u00b6 Kuartil adalah nilai atau angka yang membagi data dalam empat bagian yang sama, setelah diurut dari terkecil sampai terbesar. Q1 untuk quartile bawah. Q1 ini mempunyai nilai 25% dari data. Q2 atau biasa disebut quartile tengah yang mempunyai nilai sama seperti median yaitu 50% dari data. dan Q3 sebagai quartile atas yang mempunyai nilai 75% dari data. $$ Q_1 = (n + 1) {1\\over 4} \\ Q_2 = (n + 1) {1\\over 2} \\ Q_3 = (n + 1) {3\\over 4} $$ Langkah Langkah Masukkan Library yang telah dibuat from scipy import stats import numpy as np import seaborn as sns import matplotlib as plt import pandas as pd Panggil dan tampilkan data csv df = pd.read_csv('pd.csv') df NOMOR ANTRIAN PENYAKIT JANTUNG PENYAKIT HATI PENYAKIT GINJAL PENYAKIT MATA 0 1001 93 73 89 88 1 1002 78 78 70 95 2 1003 67 81 73 77 3 1004 87 60 76 68 4 1005 90 90 66 52 ... ... ... ... ... ... 495 1496 95 65 79 56 496 1497 94 61 82 80 497 1498 87 88 54 71 498 1499 59 87 99 63 499 1500 86 98 75 65 500 rows \u00d7 5 columns Mengambil data dari beberapa kolom pada csv dan dihitung oleh pandas itu . kemudian hasil akan di disimpan pada penyimpanan tadi dan ditampilkan hasil perhitungan tersebut. #code from IPython.display import HTML,display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"describe()\"]+['<pre>'+str(df[col].describe())+'</pre>' for col in df.columns], [\"count()\"]+[df[col].count()for col in df.columns], [\"mean()\"]+[df[col].mean()for col in df.columns], [\"std()\"]+[\"{:.2f}\".format(df[col].std())for col in df.columns], [\"min()\"]+[df[col].min() for col in df.columns], [\"max()\"]+[df[col].max() for col in df.columns], [\"q1()\"]+[df[col].quantile(0.25)for col in df.columns], [\"q2()\"]+[df[col].quantile(0.50)for col in df.columns], [\"q3()\"]+[df[col].quantile(0.75)for col in df.columns], [\"skew()\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table,tablefmt='html'))) method NOMOR ANTRIAN PENYAKIT JANTUNG PENYAKIT HATI PENYAKIT GINJAL PENYAKIT MATA describe() count 500.000000 mean 1250.500000 std 144.481833 min 1001.000000 25% 1125.750000 50% 1250.500000 75% 1375.250000 max 1500.000000 Name: NOMOR ANTRIAN, dtype: float64 count 500.000000 mean 75.930000 std 14.534635 min 50.000000 25% 64.000000 50% 77.000000 75% 88.000000 max 100.000000 Name: PENYAKIT JANTUNG, dtype: float64 count 500.000000 mean 74.778000 std 14.615606 min 50.000000 25% 62.000000 50% 76.000000 75% 87.000000 max 100.000000 Name: PENYAKIT HATI, dtype: float64 count 500.000000 mean 75.184000 std 14.343836 min 50.000000 25% 63.750000 50% 75.000000 75% 87.000000 max 100.000000 Name: PENYAKIT GINJAL, dtype: float64 count 500.000000 mean 75.658000 std 14.823242 min 50.000000 25% 63.000000 50% 77.000000 75% 89.000000 max 100.000000 Name: PENYAKIT MATA, dtype: float64 count() 500 500 500 500 500 mean() 1250.5 75.93 74.778 75.184 75.658 std() 144.48 14.53 14.62 14.34 14.82 min() 1001 50 50 50 50 max() 1500 100 100 100 100 q1() 1125.75 64.0 62.0 63.75 63.0 q2() 1250.5 77.0 76.0 75.0 77.0 q3() 1375.25 88.0 87.0 87.0 89.0 skew() 0.00 -0.09 -0.05 -0.03 -0.10 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Statistik Deskriptif"},{"location":"keterangan/#meanrata-rata","text":"Mean adalah rata-rata dari semua angka , yang didapat dengan menjumlahkan data seluruh individu dalam kelompok itu,kemudian dibagi dengan jumlah individu yang ada pada kelompok tersebut. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ x bar = Rata-rata x = Data ke-n n = Banyaknya data","title":"Mean(Rata-rata)"},{"location":"keterangan/#median","text":"Median adalah nilai pusat yang terletak ditengah data yang telah diurutkan dari terkecil sampai terbesar. \u200b untuk data tunggal : $$ Me=\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap \\ $$ \u200b untuk data kelompok : $$ M e=x_{i j}+\\left(\\frac{\\frac{n}{2}-f_{k i j}}{f_{i}}\\right) p $$ Me = Median xi = batas bawah median n = jumlah data fkij = frekuensi kumulatif data di bawah kelas median fi = frekuensi data pada kelas median p = panjang interval kelas","title":"Median"},{"location":"keterangan/#modus","text":"Modus adalah data yang paling banyak ditemukan atau nilai yang sering muncul (frekuensi terbanyak). $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Mo = modus Tb = tepi bawah p = panjang interval b1 = selisih frekuensi antara elemen modus dengan elemen sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya","title":"Modus"},{"location":"keterangan/#standar-deviasi-varian","text":"Standar deviasi adalah nilai statistik yang digunakan untuk menentukan bagaimana sebaran data dalam sampel, dan seberapa dekat titik data individu pada rata-rata. Nilai sampel Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Varian adalah jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ xi = titik data x bar = Rata-rata n = banyak nya data","title":"Standar Deviasi &amp; Varian"},{"location":"keterangan/#skewness","text":"Skewness adalah derajat ketidaksamaan atau ukuran kemiringan suatu distribusi yang digunakan untuk menentukan sejauhmana perbedaan suatu distribusi dengan distribusi normal. $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ xi = titik data x bar = Rata-rata dari distribusi n = banyak titik dalam distribusi o = standar deviasi","title":"Skewness"},{"location":"keterangan/#kuartil","text":"Kuartil adalah nilai atau angka yang membagi data dalam empat bagian yang sama, setelah diurut dari terkecil sampai terbesar. Q1 untuk quartile bawah. Q1 ini mempunyai nilai 25% dari data. Q2 atau biasa disebut quartile tengah yang mempunyai nilai sama seperti median yaitu 50% dari data. dan Q3 sebagai quartile atas yang mempunyai nilai 75% dari data. $$ Q_1 = (n + 1) {1\\over 4} \\ Q_2 = (n + 1) {1\\over 2} \\ Q_3 = (n + 1) {3\\over 4} $$ Langkah Langkah Masukkan Library yang telah dibuat from scipy import stats import numpy as np import seaborn as sns import matplotlib as plt import pandas as pd Panggil dan tampilkan data csv df = pd.read_csv('pd.csv') df NOMOR ANTRIAN PENYAKIT JANTUNG PENYAKIT HATI PENYAKIT GINJAL PENYAKIT MATA 0 1001 93 73 89 88 1 1002 78 78 70 95 2 1003 67 81 73 77 3 1004 87 60 76 68 4 1005 90 90 66 52 ... ... ... ... ... ... 495 1496 95 65 79 56 496 1497 94 61 82 80 497 1498 87 88 54 71 498 1499 59 87 99 63 499 1500 86 98 75 65 500 rows \u00d7 5 columns Mengambil data dari beberapa kolom pada csv dan dihitung oleh pandas itu . kemudian hasil akan di disimpan pada penyimpanan tadi dan ditampilkan hasil perhitungan tersebut. #code from IPython.display import HTML,display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"describe()\"]+['<pre>'+str(df[col].describe())+'</pre>' for col in df.columns], [\"count()\"]+[df[col].count()for col in df.columns], [\"mean()\"]+[df[col].mean()for col in df.columns], [\"std()\"]+[\"{:.2f}\".format(df[col].std())for col in df.columns], [\"min()\"]+[df[col].min() for col in df.columns], [\"max()\"]+[df[col].max() for col in df.columns], [\"q1()\"]+[df[col].quantile(0.25)for col in df.columns], [\"q2()\"]+[df[col].quantile(0.50)for col in df.columns], [\"q3()\"]+[df[col].quantile(0.75)for col in df.columns], [\"skew()\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table,tablefmt='html'))) method NOMOR ANTRIAN PENYAKIT JANTUNG PENYAKIT HATI PENYAKIT GINJAL PENYAKIT MATA describe() count 500.000000 mean 1250.500000 std 144.481833 min 1001.000000 25% 1125.750000 50% 1250.500000 75% 1375.250000 max 1500.000000 Name: NOMOR ANTRIAN, dtype: float64 count 500.000000 mean 75.930000 std 14.534635 min 50.000000 25% 64.000000 50% 77.000000 75% 88.000000 max 100.000000 Name: PENYAKIT JANTUNG, dtype: float64 count 500.000000 mean 74.778000 std 14.615606 min 50.000000 25% 62.000000 50% 76.000000 75% 87.000000 max 100.000000 Name: PENYAKIT HATI, dtype: float64 count 500.000000 mean 75.184000 std 14.343836 min 50.000000 25% 63.750000 50% 75.000000 75% 87.000000 max 100.000000 Name: PENYAKIT GINJAL, dtype: float64 count 500.000000 mean 75.658000 std 14.823242 min 50.000000 25% 63.000000 50% 77.000000 75% 89.000000 max 100.000000 Name: PENYAKIT MATA, dtype: float64 count() 500 500 500 500 500 mean() 1250.5 75.93 74.778 75.184 75.658 std() 144.48 14.53 14.62 14.34 14.82 min() 1001 50 50 50 50 max() 1500 100 100 100 100 q1() 1125.75 64.0 62.0 63.75 63.0 q2() 1250.5 77.0 76.0 75.0 77.0 q3() 1375.25 88.0 87.0 87.0 89.0 skew() 0.00 -0.09 -0.05 -0.03 -0.10 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Kuartil"}]}